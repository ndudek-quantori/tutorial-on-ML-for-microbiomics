{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive tutorial: best practices for supervised machine learning in microbiomics\n",
    "\n",
    "This Colab notebook is an accompaniment to the paper \"Best practices for supervised machine learning in microbiomics\" (**ADD CITATION**).\n",
    "\n",
    "In this tutorial, we will develop a machine learning (ML) classifier that can predict whether an individual has schizophrenia based on the composition of their fecal microbiota. Data was sourced from [Zhu, Feng, et al. \"Metagenome-wide association of gut microbiome features for schizophrenia.\" Nature communications 11.1 (2020): 1612.](https://www.nature.com/articles/s41467-020-15457-9) For more information about the clinical trial under which data was collected, including the study design and eligibility criteria, see [clinicaltrials.gov, identifier NCT02708316](https://clinicaltrials.gov/ct2/show/NCT02708316?term=NCT02708316&draw=2&rank=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're new to Colab:\n",
    "\n",
    "- To run this notebook, you'll need to make your own copy by going to File > Save a copy in Drive.\n",
    "- Code in cells can be run by clicking the play button on the lefthand side of the cell with your cursor or by using the keyboard shortcut \"Command/Ctrl+Enter\".\n",
    "- To learn more about Colab, check out the [documentation](https://colab.research.google.com/#scrollTo=-Rh3-Vt9Nev9) and [FAQ](https://research.google.com/colaboratory/faq.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically upload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from ml4microbiome import data_visualization, pre_process, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./\"  # change to your path\n",
    "\n",
    "DATA_PATH = \"zhu_metaphlan_read_counts.txt\"\n",
    "METADATA_PATH = \"zhu_supplemental.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental design\n",
    "\n",
    "**Goals:** \n",
    "1. Build an ML classifier that can distinguish between individuals with vs without schizophrenia using microbiome and/or clinical data. \n",
    "2. Understand why/how the model works.\n",
    "3. Investigate which population segments it performs more or less well on. \n",
    "\n",
    "The data will be pre-processed as follows. While some may be more or less important for different projects and methodologies (e.g.: feature selection), we perform them here to illustrate how they can be incorporated into experimental design without data leakage. \n",
    "- CLR transform microbiome data\n",
    "- Imputation (**add type**) of missing metadata entries\n",
    "- Feature scaling (**add type**) on numerical metadata features\n",
    "- Feature selection (**add type**)\n",
    "\n",
    "We will compare the performance of models trained using the following parameters:\n",
    "- Different types of features (metadata only, microbiome only, combined)\n",
    "- Different ML algorithms (random forest, L1 logistic regression, LightGBM)\n",
    "- Different taxonomic levels (species, genus, family, all taxonomic levels)\n",
    "\n",
    "These comparisons will be performed using repeated, nested cross-validation (CV). Importantly, model selection will be performed on a nested CV set so that the performance of the final model will be evaluated on previously unseen data. \n",
    "\n",
    "Once our best model is selected, we will endeavour to explain how/why it makes predictions using Shapley Additive Explanations (SHAP). \n",
    "\n",
    "Finally, we will perform an error analysis to better understand under what circumstances our model may perform more or less well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Raw reads were downloaded from the [European Nucleotide Archive Project PRJEB29127](https://www.ebi.ac.uk/ena/browser/view/PRJEB29127). Quality control analysis with FastQC v0.11.9 (Andrews, 2010) and MultiQC v1.12 (Ewels *et al*., 2016), revealing that reads in the dataset had high quality scores across their lengths and that there was minimal adapter content remaining. As such, we proceeded to remove human contamination by mapping reads against human reference genome [GRCh38.p14](https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000001405.40/) using bowtie2 vX (Langmead & Salzberg, 2012). \n",
    "\n",
    "The taxonomic composition of samples was inferred  using [Metaphlan4](https://huttenhower.sph.harvard.edu/metaphlan/) (Segata *et al*., 2012; Blanco-Miguez *et al*., 2022) and the Jan 2021 ChocoPhlAn3 database as a reference (citation). Metaphlan4 was run with the -t rel_ab_w_read_stats flag to obtain read counts per taxon - this will later be normalized using the CLR transformation. One of the strengths of shotgun sequencing is that it can be used to profile the bacteria, archaea, eukaryotes, and viruses in a sample, all of which can influence community ecology and host phenotype. By default, Metaphlan4 will profile bacterial, archaeal, and eukaryotic taxa. We added the --add_viruses flag to include information about viral taxa present in metagenomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best practices:**\n",
    "\n",
    "- For software tools, databases, and reference genomes, note the version used. In rare instances where the distributor does not specify explicit versions, state the date on which you accessed them.\n",
    "- For software tools, describe whether you used default parameters vs customized settings. If the latter, explain which settings you used and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR + DATA_PATH, sep=\"\\t\", header=1, index_col=0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is essential that ML practitioners understand the format of raw input data before developing models. Here, we are working with the profile file output by Metaphlan4. \n",
    "\n",
    "Each column represents a sample and each row represents a taxon that was identified in the dataset. Values in cells represent the number of reads per sample attributed to a given taxon. For every taxonomic level (kingdom, phylum, ... genus, species), metaphlan outputs the read counts of all taxa identified at that level. Taxonomic levels are denoted by \"k__\" for kingdom, \"p__\" for phylum, ..., \"g__\" for genus, \"s__\" for species. This means that some rows will be redundant. For example, reads asigned to the species *Escherichia coli* will also be counted under genus *Escherichia*, family Enterobacteriaceae, Order Enterobacterales, Phylum Pseudomonadota, and Kingdom Bacteria. \n",
    "\n",
    "For more information, see the [Metaphlan4 tutorial](https://github.com/biobakery/biobakery/wiki/metaphlan4#131-the-metaphlan-taxonomic-profile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.read_csv(\n",
    "    BASE_DIR + METADATA_PATH, skiprows=3, index_col=0, sep=\"\\t\"\n",
    ").iloc[:, :-4]\n",
    "df_metadata = df_metadata.replace(\n",
    "    to_replace=\"Xi'An Mental Health Center\", value=\"Xi'an\"\n",
    ")\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the metadata, a couple things to note are:\n",
    "- Some metadata columns contain NaNs, we will perform imputation to deal with those later. Importantly, imputation must be done after the training-test split for model evaluation, so as to avoid data leakage.\n",
    "- Categorical features such as \"Gender\" should be one hot encoded.\n",
    "- Behavioural features may be affected by whether or not an individual is aware that they have diagnosed schizophrenia. For example, an individual with severe schizophrenia may be more or less likely to live in a certain type of housing (\"Dwelling condition\"). Since we want to develop a classifier that would theoretically be able to predict whether individuals without diagnosed schizophrenia may have schizophrenia, we will not include behavioural features in our ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best practices:**  \n",
    "- Understand the format of data and metadata that will be used as input to the ML experiment\n",
    "- Carefully consider whether metadata could include features that can result in data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before designing our ML experiment, we will perform data exploration to understand the dataset available. \n",
    "\n",
    "This includes characterizing:\n",
    "- Dataset size\n",
    "- Demographics of the population sampled\n",
    "- Technical features relating to how the data was generated (e.g.: collection, sampling, and processing methods)\n",
    "- Properties of the sequenced dataset, such as the # of species identified by Metaphlan4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: let's not do any data exploration until we see what the ML results look like\n",
    "# and if we want to use this dataset for the final tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = data_visualization.pie_plot_hospital(df_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: data exploration and visualization\n",
    "# E.g.: participant demographics (age, sex)\n",
    "# E.g.: distribution of patients (and diagnoses) per hospital center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data \n",
    "\n",
    "To account for the compositionality of microbiomics data, we will apply the centered log ratio (CLR) transformation. We will not use relative abundance because ____. To avoid data leakage, other types of data pre-processing (feature scaling, feature selection) must be applied only once the training-test split has been made for evaluating model performance. The difference between CLR vs something like feature scaling is that CLR is calculate per sample, whereas the statistics used for feature scaling are calculated across multiple samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_levels = [\"all\", \"species\", \"genus\", \"family\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = defaultdict(lambda: defaultdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dfs with specific taxonomic levels (e.g.: species only)\n",
    "# CLR transform microbiome datasets\n",
    "for tax_level in tax_levels[1:]:\n",
    "    _ = pre_process.get_tax_level(df, tax_level)\n",
    "    data_dict[\"microbiome_only\"][tax_level] = pre_process.clr_transform(_).T\n",
    "\n",
    "data_dict[\"microbiome_only\"][\"all\"] = pre_process.clr_transform(df).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format target labels\n",
    "# Target = whether or not a study participant has schizophrenia\n",
    "y = df_metadata[\"Group\"].replace(to_replace=\"HC\", value=\"NS\")  # NS = No schizophrenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For metadata, we will retain only features that derive from objective clinical measurements\n",
    "X_metadata_only = df_metadata[\n",
    "    [\n",
    "        \"Age\",\n",
    "        \"BMI\",\n",
    "        \"Height(cm)\",\n",
    "        \"Weight(kg)\",\n",
    "        \"Pulse(c. p. m)\",\n",
    "        \"Breathe(c. p. m)\",\n",
    "        \"Systolic pressure(mmHg)\",\n",
    "        \"Diastolic pressure(mmHg)\",\n",
    "        \"Tryptophane(μM)\",\n",
    "        \"Glutamic acid(μM)\",\n",
    "        \"Tyrosine(μM)\",\n",
    "        \"Phenylalanine(μM)\",\n",
    "        \"Dopamine(ng/ml)\",\n",
    "        \"Gamma-aminobutyric acid  (GABA)(ng/L)\",\n",
    "        \"Serotonin(ng/ml)\",\n",
    "        \"Kynurenine (KYN)(nmol/L)\",\n",
    "        \"Kynurenic acid (KYNA)(nmol/L)\",\n",
    "    ]\n",
    "]\n",
    "metadata_continuous_cols = X_metadata_only.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll one hot encode \"Gender\" -- a categorical feature\n",
    "# rows are samples, columns are features\n",
    "gender = pd.get_dummies(df_metadata[\"Gender (1:male, 2:female)\"])\n",
    "X_metadata_only = pd.merge(X_metadata_only, gender, left_index=True, right_index=True)\n",
    "data_dict[\"metadata_only\"][\"all\"] = X_metadata_only\n",
    "\n",
    "#### TODO: add headers back in for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge metadata with microbiome data (at different taxonomic levels)\n",
    "data_dict = pre_process.merge_metadata_microbiome(data_dict, tax_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform feature selection, model training, and hyperparameter tuning using nested 10-fold cross-validation\n",
    "\n",
    "[Add text explaining strengths of our experimental design]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Section on choosing k for k-fold CV\n",
    "# Talk about why you might want k=5 vs k=10, some combination thereof, etc\n",
    "# Think through how many samples you have, and how many of each class\n",
    "# Here, 171 samples - 81 HC, 90 SCZ (df_metadata[\"Group\"].value_counts())\n",
    "# Stratified 10-fold CV:\n",
    "#   17*9=153 for training fold, 17 samples for test fold\n",
    "# When nested, with 5-fold CV:\n",
    "#   153/5 = 30 ---> 30*4=120 for training, 30 for test -- ~15 HC and 15 SCH\n",
    "# For a smaller dataset, however, 10-fold CV might not make sense,\n",
    "#   especially if there is a strong class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record time and memory usage to record models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = pd.Series(encoder.fit_transform(y))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_types = [\"metadata_only\", \"microbiome_only\", \"metadata_microbiome\"]\n",
    "learning_algs = [\"random_forest\", \"lightGBM\", \"logistic_regression_L1\"]\n",
    "reps = 10\n",
    "test_size = 0.2\n",
    "params_distributions = {\n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": np.arange(50, 550, 50),\n",
    "        \"max_features\": [\"log2\", \"sqrt\", None],\n",
    "        \"min_samples_split\": np.arange(2, 10, 2),\n",
    "        \"min_samples_leaf\": np.arange(2, 12, 2),\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"max_depth\": np.arange(5, 11),\n",
    "    },\n",
    "    \"lightGBM\": {\n",
    "        \"learning_rate\": np.linspace(0.01, 0.5, 50),\n",
    "        \"objective\": [\"binary\"],\n",
    "        \"boosting_type\": [\"gbdt\"],\n",
    "        \"num_leaves\": np.arange(25, 200, 25),\n",
    "        \"max_depth\": np.arange(1, 10),\n",
    "        \"subsample\": np.arange(0.5, 1.0, 0.1),\n",
    "        \"subsample_freq\": np.arange(2, 10, 2),\n",
    "        \"colsample_bytree\": np.arange(0.5, 1.0, 0.1),\n",
    "        \"min_child_weight\": np.arange(25, 100, 25),\n",
    "        \"reg_alpha\": np.arange(25, 100, 25),\n",
    "        \"max_bin\": np.arange(155, 355, 50),\n",
    "        \"min_child_samples\": np.arange(10, 50, 10),\n",
    "    },\n",
    "    \"logistic_regression_L1\": {\n",
    "        \"max_iter\": [10000],\n",
    "        \"penalty\": [\"l1\"],\n",
    "        \"C\": np.linspace(0.1, 100, 50),\n",
    "        \"solver\": [\"liblinear\", \"saga\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "outer_results = defaultdict(\n",
    "    lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    ")\n",
    "inner_results = defaultdict(\n",
    "    lambda: defaultdict(\n",
    "        lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    )\n",
    ")\n",
    "\n",
    "for data_type in data_types:\n",
    "    for tax_level in tax_levels:\n",
    "        for alg in learning_algs:\n",
    "            # For metadata_only, there are no taxonomic levels so only train once\n",
    "            if data_type == \"metadata_only\" and tax_level != \"all\":\n",
    "                continue\n",
    "\n",
    "            print(f\"Data type: {data_type}\")\n",
    "            print(f\"Taxonomic level: {tax_level}\")\n",
    "            print(f\"Algorithm: {alg}\")\n",
    "\n",
    "            for outer_iter_no in range(reps):\n",
    "                # Define outer loop training and test sets\n",
    "                (\n",
    "                    outer_X_train,\n",
    "                    outer_X_test,\n",
    "                    outer_y_train,\n",
    "                    outer_y_test,\n",
    "                ) = train_test_split(\n",
    "                    data_dict[data_type][tax_level],\n",
    "                    y_encoded,\n",
    "                    test_size=test_size,\n",
    "                    stratify=y_encoded,\n",
    "                    random_state=outer_iter_no,\n",
    "                )\n",
    "\n",
    "                # Median imputation based on outer cv set\n",
    "                # To avoid data leakage:\n",
    "                #     Calculate median values on the TRAINING set\n",
    "                #     Use TRAINING set medians to impute TRAINING and TEST set missing values\n",
    "                outer_X_train = outer_X_train.fillna(outer_X_train.median())\n",
    "                outer_X_test = outer_X_test.fillna(outer_X_train.median())\n",
    "\n",
    "                # Feature scaling based on outer cv set\n",
    "                # (Probably not actually that helpful here, but good for demonstration purposes)\n",
    "                # To avoid data leakage:\n",
    "                #    Calculate scaling stats on the TRAINING set alone\n",
    "                #    Use TRAINING set stats to scale TRAINING and TEST set values\n",
    "                if data_type != \"microbiome_only\":\n",
    "                    outer_X_train, outer_X_test = train.scale_features(\n",
    "                        outer_X_train, outer_X_test, metadata_continuous_cols\n",
    "                    )\n",
    "\n",
    "                # To do: add feature selection step\n",
    "\n",
    "                # Store AUROCs for inner test set\n",
    "                # And the best hyper-parameters\n",
    "                for inner_iter_no in range(reps):\n",
    "                    # Define inner loop training and test sets\n",
    "                    (\n",
    "                        inner_X_train,\n",
    "                        inner_X_test,\n",
    "                        inner_y_train,\n",
    "                        inner_y_test,\n",
    "                    ) = train_test_split(\n",
    "                        outer_X_train,\n",
    "                        outer_y_train,\n",
    "                        test_size=test_size,\n",
    "                        stratify=outer_y_train,\n",
    "                        random_state=inner_iter_no,\n",
    "                    )\n",
    "\n",
    "                    best_model, best_params = train.tune_model(\n",
    "                        alg,\n",
    "                        params_distributions[alg],\n",
    "                        \"roc_auc\",\n",
    "                        inner_X_train,\n",
    "                        inner_y_train,\n",
    "                        inner_iter_no,\n",
    "                    )\n",
    "                    auc = train.test_model(best_model, inner_X_test, inner_y_test)\n",
    "                    inner_results[data_type][tax_level][alg][outer_iter_no][\n",
    "                        \"AUROC\"\n",
    "                    ].append(auc)\n",
    "                    inner_results[data_type][tax_level][alg][outer_iter_no][\n",
    "                        \"best_params\"\n",
    "                    ].append(best_params)\n",
    "\n",
    "                # Back to outer training-test split\n",
    "                # For a given data_type, tax_level, and alg, identify the best model stored in inner_results\n",
    "                aurocs_np = np.array(\n",
    "                    inner_results[data_type][tax_level][alg][outer_iter_no][\"AUROC\"]\n",
    "                )\n",
    "                median_auroc = np.median(aurocs_np)\n",
    "                max_auroc_index = aurocs_np.argmax()\n",
    "                params = inner_results[data_type][tax_level][alg][outer_iter_no][\n",
    "                    \"best_params\"\n",
    "                ][max_auroc_index]\n",
    "\n",
    "                outer_results[data_type][tax_level][alg][\n",
    "                    \"inner_median_valid_AUROC\"\n",
    "                ].append(median_auroc)\n",
    "                outer_results[data_type][tax_level][alg][\"params\"].append(params)\n",
    "\n",
    "                # Train a model with the best parameters\n",
    "                model = train.train_model(\n",
    "                    alg, outer_X_train, outer_y_train, outer_iter_no, params\n",
    "                )\n",
    "\n",
    "                # Evaluate and record its performance on the outer test set in outer_results\n",
    "                auc = train.test_model(model, outer_X_test, outer_y_test)\n",
    "                outer_results[data_type][tax_level][alg][\"AUROC\"].append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select best model based on CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each trained model, calculate median CV result plus standard deviation\n",
    "# Use median AUROC to select best model\n",
    "\n",
    "# Find maximum median AUROC\n",
    "max_median_auroc = 0\n",
    "\n",
    "for data_type_key, data_type_value in outer_results.items():\n",
    "    for tax_level_key, tax_level_value in data_type_value.items():\n",
    "        for alg_key, alg_value in tax_level_value.items():\n",
    "            max_auroc = max(alg_value[\"inner_median_valid_AUROC\"])\n",
    "            if max_auroc > max_median_auroc:\n",
    "                max_median_auroc = max_auroc\n",
    "\n",
    "max_median_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find all models that have max_median_auroc\n",
    "\n",
    "best_models_info = []\n",
    "tolerance = 1e-6  # 0.000001\n",
    "\n",
    "for data_type_key, data_type_value in outer_results.items():\n",
    "    for tax_level_key, tax_level_value in data_type_value.items():\n",
    "        for alg_key, alg_value in tax_level_value.items():\n",
    "            for ind, value in enumerate(alg_value[\"inner_median_valid_AUROC\"]):\n",
    "                if abs(value - max_median_auroc) < tolerance:\n",
    "                    best_models_info.append(\n",
    "                        {\n",
    "                            \"data_type\": data_type_key,\n",
    "                            \"tax_level\": tax_level_key,\n",
    "                            \"alg\": alg_key,\n",
    "                            \"params\": alg_value[\"params\"][ind],\n",
    "                            \"random_state\": ind,\n",
    "                            \"outer_iter_no\": ind,\n",
    "                            \"test_AUROC\": alg_value[\"AUROC\"][ind],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "best_models_info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance of the best model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already calculated the performance of each model on the test set\n",
    "# Access those scores for the best model\n",
    "for num, model_info in enumerate(best_models_info, 1):\n",
    "    print(f\"#{num}: \")\n",
    "    print(f\"\\tData Type: {model_info['data_type']}\")\n",
    "    print(f\"\\tTaxonomic Level: {model_info['tax_level']}\")\n",
    "    print(f\"\\tLearning Algorithm: {model_info['alg']}\")\n",
    "    print(f\"\\tAUROC: {model_info['test_AUROC']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance of the best model on an external validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g.: Does the model perform better or worse on people of different ages, sex\n",
    "# E.g.: Does the model have some sort of bias based on hospital center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that the work is reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add talking points about how to make the work reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Provide example of what to report / language to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works Cited\n",
    "\n",
    "Andrews, S. \"FastQC: a quality control tool for high throughput sequence data.\" (2010). Available online at: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/.\n",
    "\n",
    "Ewels, P., Magnusson, M., Lundin, S., Käller, M. \"MultiQC: summarize analysis results for multiple tools and samples in a single report.\" *Bioinformatics* 32.19 (2016): 3047-3048.\n",
    "\n",
    "Zhu, F., *et al*. \"Metagenome-wide association of gut microbiome features for schizophrenia.\" *Nature communications* 11.1 (2020): 1612.\n",
    "\n",
    "Langmead, B., Salzberg, S.L. \"Fast gapped-read alignment with Bowtie 2.\" *Nature methods* 9.4 (2012): 357-359."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
